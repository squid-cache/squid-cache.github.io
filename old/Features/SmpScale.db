<?xml version="1.0" encoding="utf-8"?><!DOCTYPE article  PUBLIC '-//OASIS//DTD DocBook XML V4.4//EN'  'http://www.docbook.org/xml/4.4/docbookx.dtd'><article><articleinfo><title>Features/SmpScale</title><revhistory><revision><revnumber>43</revnumber><date>2016-02-23 22:11:43</date><authorinitials>AlexRousskov</authorinitials><revremark>Added SMP-related configuration suggestions for top performance.</revremark></revision><revision><revnumber>42</revnumber><date>2015-03-28 10:38:33</date><authorinitials>AmosJeffries</authorinitials></revision><revision><revnumber>41</revnumber><date>2014-07-17 00:39:29</date><authorinitials>NyamulHassan</authorinitials></revision><revision><revnumber>40</revnumber><date>2013-10-28 15:53:59</date><authorinitials>AlexRousskov</authorinitials><revremark>Fixed cache_dirs link.</revremark></revision><revision><revnumber>39</revnumber><date>2013-10-28 15:51:24</date><authorinitials>AlexRousskov</authorinitials><revremark>Explained how many Squid processes to expect.</revremark></revision><revision><revnumber>38</revnumber><date>2013-10-28 15:44:17</date><authorinitials>AlexRousskov</authorinitials><revremark>Documented terminology.</revremark></revision><revision><revnumber>37</revnumber><date>2013-08-28 00:23:00</date><authorinitials>AlexRousskov</authorinitials><revremark>Documented an ICP workaround. Fixed punctuation.</revremark></revision><revision><revnumber>36</revnumber><date>2013-08-28 00:12:45</date><authorinitials>AlexRousskov</authorinitials><revremark>Clarified ICP/HTCP sharing problems and mention them more prominently.</revremark></revision><revision><revnumber>35</revnumber><date>2013-08-09 03:33:45</date><authorinitials>AmosJeffries</authorinitials><revremark>mention ICP/HTCP issue which came to light this week</revremark></revision><revision><revnumber>34</revnumber><date>2013-04-02 16:51:38</date><authorinitials>AlexRousskov</authorinitials><revremark>Listed SNMP stats as SMP-shared.</revremark></revision><revision><revnumber>33</revnumber><date>2013-04-01 14:57:22</date><authorinitials>AlexRousskov</authorinitials><revremark>Fixed column description. Typo reported by Suren Karapetyan.</revremark></revision><revision><revnumber>32</revnumber><date>2013-03-28 14:48:33</date><authorinitials>AlexRousskov</authorinitials><revremark>Added stateful HTTP authentication to the list of SMP-unaware features (bug 3517)</revremark></revision><revision><revnumber>31</revnumber><date>2013-03-28 14:43:13</date><authorinitials>AlexRousskov</authorinitials><revremark>Added more details about SMP effects on SMP-unaware features. Mentioned that SSL session sharing is in the works.</revremark></revision><revision><revnumber>30</revnumber><date>2013-03-28 14:29:49</date><authorinitials>AlexRousskov</authorinitials><revremark>Added delay pools to the list of SMP-unaware features.</revremark></revision><revision><revnumber>29</revnumber><date>2013-03-28 14:26:36</date><authorinitials>AlexRousskov</authorinitials><revremark>Made list item delimiters consistent.</revremark></revision><revision><revnumber>28</revnumber><date>2013-02-24 17:57:33</date><authorinitials>AlexRousskov</authorinitials><revremark>Polished &quot;Cannot bind socket&quot; tips further: the admin may not use --localstatedir (but may use --prefix which affects localstatedir).</revremark></revision><revision><revnumber>27</revnumber><date>2013-02-23 00:54:49</date><authorinitials>AlexRousskov</authorinitials><revremark>Polihsed &quot;Cannot bind socket&quot; tips after bug 3789: Mentioned --localstatedir. Removed reference to PID file because it was confusing admins using --with-pidfile. Spellchecked that paragraph.</revremark></revision><revision><revnumber>26</revnumber><date>2013-01-10 16:51:09</date><authorinitials>AlexRousskov</authorinitials><revremark>Noted that secure cache manager requests are not currently supported. To be detailed on the CacheManager page.</revremark></revision><revision><revnumber>25</revnumber><date>2012-10-12 18:11:43</date><authorinitials>AlexRousskov</authorinitials><revremark>Documented &quot;message too long&quot; troubleshooting.</revremark></revision><revision><revnumber>24</revnumber><date>2012-09-22 16:34:21</date><authorinitials>AlexRousskov</authorinitials><revremark>Added a link to SMP Cache Manager details.</revremark></revision><revision><revnumber>23</revnumber><date>2012-09-21 17:25:15</date><authorinitials>AlexRousskov</authorinitials><revremark>Added ToC.</revremark></revision><revision><revnumber>22</revnumber><date>2012-09-19 00:45:47</date><authorinitials>AmosJeffries</authorinitials><revremark>add toubleshooting for some system configuration issues that are coming into view.</revremark></revision><revision><revnumber>21</revnumber><date>2012-09-12 16:48:45</date><authorinitials>AlexRousskov</authorinitials><revremark>Documented that workers do not share SSL session cache so session resumption does not work reliably.</revremark></revision><revision><revnumber>20</revnumber><date>2012-04-05 15:08:33</date><authorinitials>AlexRousskov</authorinitials><revremark>Clarified current object cache sharing</revremark></revision><revision><revnumber>19</revnumber><date>2011-02-15 07:16:06</date><authorinitials>AlexRousskov</authorinitials><revremark>polished table headings</revremark></revision><revision><revnumber>18</revnumber><date>2011-02-15 07:09:42</date><authorinitials>AlexRousskov</authorinitials><revremark>Documented worker load balancing problem and workaround</revremark></revision><revision><revnumber>17</revnumber><date>2010-12-01 16:03:05</date><authorinitials>AlexRousskov</authorinitials><revremark>polished the last section to add details and to make sure readers do not misinterpret the list of shareable resources as the list of things currently shared.</revremark></revision><revision><revnumber>16</revnumber><date>2010-11-30 18:12:34</date><authorinitials>AlexRousskov</authorinitials><revremark>Added &quot;What can workers share?&quot; subsection to detail the answer to that question.</revremark></revision><revision><revnumber>15</revnumber><date>2010-11-30 17:40:27</date><authorinitials>AlexRousskov</authorinitials><revremark>Isolated Coordinator info into a dedicated subsection; polished stats coordination description; polihsed section title</revremark></revision><revision><revnumber>14</revnumber><date>2010-11-30 17:10:57</date><authorinitials>AlexRousskov</authorinitials><revremark>Detailed why the &quot;dedicated acceptor&quot; design was not chosen for the initial implementation</revremark></revision><revision><revnumber>13</revnumber><date>2010-11-29 16:20:11</date><authorinitials>AlexRousskov</authorinitials><revremark>documented Coordinator to clarify that it does not participate in regular transactions</revremark></revision><revision><revnumber>12</revnumber><date>2010-11-26 02:45:23</date><authorinitials>AlexRousskov</authorinitials><revremark>updated current status; polished top architecture level to talk about the level itself rather than its current/past implementations; noted that there may be no consensus regarding the next steps/order</revremark></revision><revision><revnumber>11</revnumber><date>2010-07-07 02:33:07</date><authorinitials>AmosJeffries</authorinitials><revremark>update after SMP merge to 3.2.</revremark></revision><revision><revnumber>10</revnumber><date>2010-03-25 16:57:52</date><authorinitials>AlexRousskov</authorinitials><revremark>Updated current status in the header so that this project gets picked up in Roadmap</revremark></revision><revision><revnumber>9</revnumber><date>2010-02-19 10:29:03</date><authorinitials>FrancescoChemolli</authorinitials><revremark>Fixed dangling wiki-links</revremark></revision><revision><revnumber>8</revnumber><date>2010-02-10 06:45:09</date><authorinitials>AmosJeffries</authorinitials><revremark>link to the muti-instance config examplars by kinkie.</revremark></revision><revision><revnumber>7</revnumber><date>2009-12-16 05:24:30</date><authorinitials>AmosJeffries</authorinitials><revremark>add a summary of the overall SMP architecture discussions from squid-dev.</revremark></revision><revision><revnumber>6</revnumber><date>2009-09-06 03:42:30</date><authorinitials>AmosJeffries</authorinitials><revremark>add section on where resource locking may be a problem.</revremark></revision><revision><revnumber>5</revnumber><date>2009-09-05 03:58:49</date><authorinitials>AmosJeffries</authorinitials><revremark>link to some background work</revremark></revision><revision><revnumber>4</revnumber><date>2009-09-05 03:18:01</date><authorinitials>AmosJeffries</authorinitials><revremark>add my view of how this interacts with other feature work and one possible approach...</revremark></revision><revision><revnumber>3</revnumber><date>2008-05-18 19:38:59</date><authorinitials>localhost</authorinitials><revremark>converted to 1.6 markup</revremark></revision><revision><revnumber>2</revnumber><date>2008-03-18 16:02:21</date><authorinitials>AlexRousskov</authorinitials><revremark>This is a wish</revremark></revision><revision><revnumber>1</revnumber><date>2008-03-11 15:01:51</date><authorinitials>AlexRousskov</authorinitials><revremark>Let's make this wish explicit instead of burring it in a bag of other performance improvements</revremark></revision></revhistory></articleinfo><section><title>Feature: SMP Scalability</title><itemizedlist><listitem><para><emphasis role="strong">Goal</emphasis>: Approach linear scale in non-disk throughput with the increase of the number of processors or cores. </para></listitem><listitem><para><emphasis role="strong">Status</emphasis>: In progress; ready for deployment in some environments </para></listitem><listitem><para><emphasis role="strong">Version</emphasis>: Squid 3.2 </para></listitem><listitem><para><emphasis role="strong">Developer</emphasis>: <ulink url="https://wiki.squid-cache.org/Features/SmpScale/AlexRousskov#">AlexRousskov</ulink> </para></listitem></itemizedlist><section><title>Terminology</title><para>This sections documents SMP-related terminology. The terms and their definitions are still evolving and not all Squid documentation and developers use the same terminology. </para><itemizedlist><listitem><para><emphasis role="strong">Squid instance</emphasis>: All processes running as a result of a single &quot;squid&quot; command. This includes, but is not limited to, kid processes defined below. </para></listitem><listitem><para><emphasis role="strong">Kid</emphasis>: A Squid process (i.e., a process running Squid executable code) created by the Master process. Coordinator, worker, and diskers defined below are often Squid kids. </para></listitem><listitem><para><emphasis role="strong">Worker</emphasis>: A Squid process accepting HTTP or HTTPS requests. Workers are usually created by the Master process. In general, workers are responsible for most transaction processing but may outsource some of their work to helpers (directly), other workers (via Coordinator), or even independent servers (via ICAP, DNS, etc). </para></listitem><listitem><para><emphasis role="strong">Disker</emphasis>: A Squid process dedicated to cache_dir I/O. Diskers are created by the Master process. Today, only Rock cache_dirs may use diskers.  </para></listitem><listitem><para><emphasis role="strong">Coordinator</emphasis>: A Squid process dedicated to synchronizing other kids. </para></listitem><listitem><para><emphasis role="strong">Master</emphasis>: The first Squid process created when you run a &quot;squid&quot; command. The Master process is responsible for starting and restarting all kids. This definition is not 100% accurate because the OS creates the first process and that first Squid process then forks the actual Master process to become a daemon (except for &quot;squid -N&quot;). Since that first OS-created process exits immediately after fork, this inaccurate definition works OK for most purposes. Better wording is welcomed! </para></listitem><listitem><para><emphasis role="strong">SMP mode</emphasis>: Squid is said to be working in SMP mode when the sum of the number of worker and the number of disker processes exceeds one. Here are three random examples of a Squid instance working in SMP mode: 2 workers and 0 diskers; 1 worker and 1 disker; 2 workers and 3 diskers. Sometimes, the same &quot;SMP mode&quot; term is used to mean &quot;multiple workers&quot;; that usage excludes configurations with a single worker and multiple diskers; such usage should be avoided. </para></listitem></itemizedlist><para>Please note that the same process may play multiple roles. For example, when you start Squid with the -N command line option, there will be only one Squid process running, and that single process plays the roles of Master and Worker. </para></section><section><title>Current Status and Architecture</title><para><ulink url="https://wiki.squid-cache.org/Features/SmpScale/Squid-3.2#">Squid-3.2</ulink> supports basic SMP scale using <ulink url="http://www.squid-cache.org/Doc/config/workers#">workers</ulink>. Administrators can configure and run one Squid that spawns multiple worker processes to utilize all available CPU cores. </para><para>A worker accepts new HTTP requests and handles each accepted request until its completion. Workers can share http_ports but they do not pass transactions to each other. A worker has all capabilities of a single non-SMP Squid, but workers may be configured differently (e.g., serve different http_ports). The cpu_affinity_map option allows to dedicate a CPU core for each worker. </para><section><title>How are workers coordinated?</title><para>A special Coordinator process starts workers and coordinates their activities when needed. Here are some of the Coordinator responsibilities: </para><itemizedlist><listitem><para>restart failed worker processes; </para></listitem><listitem><para>allow workers to share listening sockets; </para></listitem><listitem><para>broadcasts reconfiguration and shutdown commands to workers; </para></listitem><listitem><para>concatenate and/or aggregate worker statistics for the Cache Manager responses. </para></listitem></itemizedlist><para>Coordinator does not participate in regular transaction handling and does not decide which worker gets to handle the incoming connection or request. The Coordinator process is usually idle. </para></section><section><title>What can workers share?</title><para>Using Coordinator and common configuration files, Squid workers can receive identical configuration information and synchronize some of their activities. By default, Squid workers share the following: </para><itemizedlist><listitem><para>Squid executable, </para></listitem><listitem><para>general configuration, </para></listitem><listitem><para>listening ports (but shared ICP and HTCP ports do not work well; see below), </para></listitem><listitem><para>logs, </para></listitem><listitem><para>memory object cache (in most environments), </para></listitem><listitem><para>disk object cache (with Rock Store only), </para></listitem><listitem><para>insecure cache manager statistics (detailed <ulink url="https://wiki.squid-cache.org/Features/SmpScale/Features/CacheManager#SMP_considerations">elsewhere</ulink>), </para></listitem><listitem><para>SNMP statistics. </para></listitem></itemizedlist><para>Cache indexes are shared without copying. Other shared information is usually small in terms of RAM use and is essentially copied to avoid locking and associated performance overheads. </para><para>Conditional configuration and worker-dependent macros can be used to limit sharing. For example, each worker can be given a dedicated http_port to listen on. </para><para>Currently, Squid workers do not share and do not synchronize other resources and services, including (but not limited to): </para><itemizedlist><listitem><para>memory object cache (in some environments), </para></listitem><listitem><para>disk object cache (except for Rock Store), </para></listitem><listitem><para>DNS caches (ipcache and fqdncache), </para></listitem><listitem><para>helper processes and daemons, </para></listitem><listitem><para>stateful HTTP authentication (e.g., digest authentication; see bug <ulink url="http://bugs.squid-cache.org/show_bug.cgi?id=3517">3517</ulink>), </para></listitem><listitem><para>delay pools, </para></listitem><listitem><para>SSL session cache (there is an active project to allow session sharing among workers), </para></listitem><listitem><para>secure cache manager statistics (detailed <ulink url="https://wiki.squid-cache.org/Features/SmpScale/Features/CacheManager#SMP_considerations">elsewhere</ulink>), </para></listitem><listitem><para>ICP/HTCP (works with a caveat: If multiple workers share the same ICP/HTCP port, an ICP/HTCP response may not go the worker that sent the request, causing timeouts at the requesting worker; use a dedicated ICP/HTCP port as a <ulink url="http://www.squid-cache.org/mail-archive/squid-users/201308/0358.html">workaround</ulink>). </para></listitem></itemizedlist><para>Some SMP-unaware features continue to work in SMP mode (e.g., DNS responses are going to be cached by individual workers), but their performance suffers from the lack of synchronization and they require more resources due to duplication of information (e.g., each worker may independently resolve and cache the IP of the same domain name). Some SMP-unaware features break badly (e.g., ufs-based cache_dirs become corrupted) unless squid.conf conditionals are used to prevent such breakage. Some SMP-unaware features will appear to work but will do so incorrectly (e.g., delay pools will limit bandwidth on per-worker basis, without sharing traffic information among workers and without dividing bandwidth limits among workers). </para></section><section><title>Why processes? Aren't threads better?</title><para>Several reasons determined the choice of processes versus threads for workers: </para><itemizedlist><listitem><para>Threading Squid code in its current shape would take too long because most of the code is thread-unsafe, including virtually all base classes. Users need SMP scale now and cannot wait for a ground-up rewrite of Squid. </para></listitem><listitem><para>Threads offer faster context switching, but in a typical SMP Squid deployment with each worker bound to a dedicated core, context switching overheads are not that important. </para></listitem><listitem><para>Both processes and threads have synchronization and sharing mechanisms sufficient for an SMP-scalable implementation. </para></listitem></itemizedlist><para>In summary, we used processes instead of threads because they allowed us to deliver similar SMP performance within reasonable time frame. Using threads was deemed not practical. </para></section><section><title>Who decides which worker gets the request?</title><para>All workers that share <ulink url="http://www.squid-cache.org/Doc/config/http_port#">http_port</ulink> listen on the same IP address and TCP port. The operating system protects the shared listening socket with a lock and decides which worker gets the new HTTP connection waiting to be accepted. Once the incoming connection is accepted by the worker, it stays with that worker. </para></section><section><title>Will similar workers receive similar amount of work?</title><para>We expected the operating system to balance the load across multiple workers by appropriately allocating the next incoming connection request to the least loaded worker/core. Worker statistics from lab tests and initial deployments proved us wrong. Here are, for example, cumulative CPU times of several identical workers handling moderate load for a while: </para><informaltable><tgroup cols="2"><colspec colname="col_0"/><colspec colname="col_1"/><tbody><row rowsep="1"><entry colsep="1" rowsep="1"><para><emphasis role="strong">Cumulative CPU</emphasis></para></entry><entry align="center" colsep="1" morerows="1" nameend="col_1" namest="col_1" rowsep="1"><para><emphasis role="strong">Worker</emphasis></para></entry></row><row rowsep="1"><entry colsep="1" rowsep="1"><para><emphasis role="strong"> Utilization (minutes)</emphasis></para></entry></row><row rowsep="1"><entry align="right" colsep="1" rowsep="1"><para>20</para></entry><entry colsep="1" rowsep="1"><para>(squid-3)</para></entry></row><row rowsep="1"><entry align="right" colsep="1" rowsep="1"><para>16</para></entry><entry colsep="1" rowsep="1"><para>(squid-4)</para></entry></row><row rowsep="1"><entry align="right" colsep="1" rowsep="1"><para>13</para></entry><entry colsep="1" rowsep="1"><para>(squid-2)</para></entry></row><row rowsep="1"><entry align="right" colsep="1" rowsep="1"><para>8</para></entry><entry colsep="1" rowsep="1"><para>(squid-1)</para></entry></row></tbody></tgroup></informaltable><para>The table shows a significant skew in worker load: squid-3 worker spent 20 minutes handling traffic while squid-1 worked for only 8 minutes. The imbalance does not improve with running time, as busiest workers remain the busiest. </para><para>While we do not know whether such imbalance results in worse response time for busier workers, it is rather undesirable from general system balance point of view. </para><para>After many days of experimenting with Squid, studying Linux TCP stack sources, and discussing the problem with other developers, we have eventually zeroed in on a relatively compact workaround with low overhead. It turns out that the first worker to be awaken to accept the new client connection is usually the worker that was the last to register its listening descriptor with epoll(2). This dependency is rather strange because the epoll sets are <emphasis>not</emphasis> shared among Squid workers; it must work on a listening socket level (those sockets <emphasis>are</emphasis> shared). Special thanks to <ulink url="https://wiki.squid-cache.org/Features/SmpScale/HenrikNordstr%C3%B6m#">HenrikNordstr√∂m</ulink> for a stimulating discussion that supplied the last missing piece of the puzzle. </para><para>The exact kernel TCP stack code responsible for this scheduling behavior is currently unknown to us and has proved difficult to find even for expert Linux kernel developers. Eventually, we may locate it and come up with a kernel module or patch to better balance listener selection in Squid environments. </para><para>However, we already have enough information for addressing the problem at Squid level. We have developed a patch that, once in a few seconds, instructs one Squid worker to delete and then immediately insert its listening descriptors from/into the epoll(2) set. Such epoll operations are relatively cheap. The patch makes sure that workers take turns at tickling their listening descriptors this way, so that only one worker becomes most active in any given tickling interval. </para><para>The change results in reasonable load distribution across workers. Here is an instant snapshot showing current CPU core utilization by each worker in addition to the total CPU time accumulated by that worker. </para><informaltable><tgroup cols="3"><colspec colname="col_0"/><colspec colname="col_1"/><colspec colname="col_2"/><tbody><row rowsep="1"><entry align="center" colsep="1" nameend="col_1" namest="col_0" rowsep="1"><para><emphasis role="strong">CPU Utilization</emphasis></para></entry><entry align="center" colsep="1" morerows="2" nameend="col_2" namest="col_2" rowsep="1"><para><emphasis role="strong">Worker</emphasis></para></entry></row><row rowsep="1"><entry colsep="1" rowsep="1"><para><emphasis role="strong">now</emphasis></para></entry><entry colsep="1" rowsep="1"><para><emphasis role="strong">cumulative</emphasis></para></entry></row><row rowsep="1"><entry colsep="1" rowsep="1"><para><emphasis role="strong">(%)</emphasis></para></entry><entry colsep="1" rowsep="1"><para><emphasis role="strong">(minutes)</emphasis></para></entry></row><row rowsep="1"><entry align="right" colsep="1" rowsep="1"><para>41</para></entry><entry align="right" colsep="1" rowsep="1"><para>2826</para></entry><entry colsep="1" rowsep="1"><para>(squid-3)</para></entry></row><row rowsep="1"><entry align="right" colsep="1" rowsep="1"><para>23</para></entry><entry align="right" colsep="1" rowsep="1"><para>2589</para></entry><entry colsep="1" rowsep="1"><para>(squid-2)</para></entry></row><row rowsep="1"><entry align="right" colsep="1" rowsep="1"><para>9</para></entry><entry align="right" colsep="1" rowsep="1"><para>2345</para></entry><entry colsep="1" rowsep="1"><para>(squid-4)</para></entry></row><row rowsep="1"><entry align="right" colsep="1" rowsep="1"><para>7</para></entry><entry align="right" colsep="1" rowsep="1"><para>2303</para></entry><entry colsep="1" rowsep="1"><para>(squid-5)</para></entry></row><row rowsep="1"><entry align="right" colsep="1" rowsep="1"><para>9</para></entry><entry align="right" colsep="1" rowsep="1"><para>2221</para></entry><entry colsep="1" rowsep="1"><para>(squid-6)</para></entry></row><row rowsep="1"><entry align="right" colsep="1" rowsep="1"><para>11</para></entry><entry align="right" colsep="1" rowsep="1"><para>2107</para></entry><entry colsep="1" rowsep="1"><para>(squid-1)</para></entry></row></tbody></tgroup></informaltable><para>The instant CPU utilization (the leftmost column) is not balanced, as expected. An administrator monitoring that column would see how the group of most active workers changes every few seconds, with new busiest workers leaving and oldest worker in the group becoming mostly idle. </para><para>The cumulative CPU time distribution (the middle column) is much more even now. Despite having to deal with seven workers, Squid shows only a 25% difference between historically most and least active worker, compared to a 60% difference among four workers without the patch. Better distribution may be possible by tuning the patch parameters such as the tickling interval or increasing worker load so that each worker is less likely to miss its chance to tickle its epoll(2) set. </para><para>We are working on making the patch compatible with non-epoll environments and will propose it for Squid v3.2 integration. </para></section><section><title>Why is there no dedicated process accepting requests?</title><para>A common alternative to the current design is a dedicated process that accepts incoming connections and gives them to one of the worker threads. We have considered and rejected this approach for the initial implementation for the following reasons: </para><itemizedlist><listitem><para>User-level scheduling and connection passing come with performance overheads. We wanted to avoid such overheads in the initial implementation so that Squid v3.2 does not become slower than earlier releases. </para></listitem><listitem><para>Since most users are not expected to treat workers differently, the OS kernel already has all the information necessary to balance the load, including low-level hardware information not available to Squid. Duplicating and/or competing with kernel CPU scheduling algorithms and tuning parameters seemed unwise in this general case. </para></listitem><listitem><para>The accepting process itself may become a bottleneck. We could support multiple accepting processes, but then the user will be facing a complex task of optimizing the number of accepting processes and the number of workers given limited number of CPU cores. Even now, without accepting processes, such optimization is complex. Since each worker is fully capable of accepting connections itself, the added complexity seemed unnecessary in the general case. </para></listitem><listitem><para>If workers are configured differently, they would require either different accepting processes or some sorts of routing maps, complicating configuration and performance optimization. </para></listitem></itemizedlist><para>A dedicated accepting process (with some additional HTTP-aware scheduling logic) may be added later, if needed. </para></section><section><title>How many processes does a single Squid instance have?</title><para>The number of Squid processes in a single Squid instance started without a -N command-line option usually is: </para><screen><![CDATA[+ 1   master process (counted only in SMP mode) plus
+ W   workers (workers in squid.conf; defaults to 1) plus
+ D   diskers (rock cache_dirs in squid.conf; defaults to 0) plus
+ 1   Coordinator process (exists only in SMP mode).]]></screen><para>For example, if you do not explicitly configure Squid <ulink url="http://www.squid-cache.org/Doc/config/workers#">workers</ulink> and rock cache_dirs, then Squid will run in non-SMP mode, and you will get 0+1+0+0=1 Squid process total. On the other hand, if you explicitly configure Squid with 3 worker and 1 rock <ulink url="http://www.squid-cache.org/Doc/config/cache_dir#">cache_dir</ulink>, then Squid will run in SMP mode, and you will get 1+3+1+1=6 Squid processes total. </para><para>The above formula does not account for helpers and other processes not running a Squid executable code. </para></section><section><title>How to configure SMP Squid for top performance?</title><para>If you have beefy hardware, want to optimize performance, and are ready to spend non-trivial amounts of time/labor/money doing that, then consider the following SMP rules of thumb: </para><orderedlist numeration="arabic"><listitem><para>If you want to cache, use the largest <ulink url="http://www.squid-cache.org/Doc/config/cache_mem#">cache_mem</ulink> your system can handle safely. Please note that Squid will not tell you when you over-allocate but may crash. If you do not want to cache, then set cache_mem to zero, prohibit caching using the <ulink url="http://www.squid-cache.org/Doc/config/cache#">cache</ulink> directive, and ignore rule #3 below. </para></listitem><listitem><para>One or two CPU core reserved for the OS, depending on network usage levels. Use OS CPU affinity configuration for network interrupts to restrict NIC interrupts to these &quot;OS-only&quot; core(s). </para></listitem><listitem><para>One Rock <ulink url="http://www.squid-cache.org/Doc/config/cache_dir#">cache_dir</ulink> per physical disk spindle with no other cache_dirs. No RAID. Diskers may be able to use virtual CPU cores. Tuning Rock is tricky. See the Performance Tuning recommendations at the Rock Store <ulink url="https://wiki.squid-cache.org/Features/SmpScale/Features/RockStore#">feature page</ulink>. Please note that compared to other cache_dir types, Rock cache_dirs are currently slower to load during Squid startup and may have other problems incompatible with your deployment needs. You may, of course, use other cache_dir types instead of Rock. These <emphasis>SMP</emphasis> rules use Rock because other cache_dirs are not SMP-aware. </para></listitem><listitem><para>One SMP <ulink url="http://www.squid-cache.org/Doc/config/workers#">worker</ulink> per remaining non-virtual CPU cores. Be wary of heavily loading multiple <emphasis>virtual</emphasis> CPU cores that share the same physical CPU core -- such virtual cores can usually accomplish less useful work than one heavily loaded physical CPU core because multiple virtual cores waste resources on competing for access to their single physical core, which is the only place where useful work happens. Virtual cores often work best for semi-idle background tasks, not busy workers with their near-real-time constraints. </para></listitem><listitem><para>Use <ulink url="http://www.squid-cache.org/Doc/config/cpu_affinity_map#">CPU affinity</ulink> for each Squid kid process (diskers and workers). Prohibit kernel from moving kids from one CPU core to another. Without your help, the general-purpose OS kernel will most likely <emphasis>not</emphasis> load-balance your Squid server well. </para></listitem><listitem><para>Watch individual CPU core utilization (not just the average or total across all cores!). Adjust the number of workers, the number of diskers, and CPU affinity maps to achieve balance while leaving a healthy overload safety margin. </para></listitem></orderedlist><para><emphasis role="strong">Disclaimer:</emphasis> YMMV. The above general rules may not apply to your environment. Following them is unlikely to be <emphasis>sufficient</emphasis> to achieve top-notch performance, especially without understanding of the underlying issues. Achieving top Squid performance on a given hardware requires a lot of work. Do not bother with this if your Squid already works OK. </para></section><section><title>Older Squids</title><para><ulink url="https://wiki.squid-cache.org/Features/SmpScale/Squid-3.1#">Squid-3.1</ulink> and older allow administrators to configure and start multiple isolated Squid instances. This labor-intensive setup allows a crude form of SMP scale in the environments where port and cache sharing are not important. Sample configurations for <ulink url="https://wiki.squid-cache.org/Features/SmpScale/Squid-3.1#">Squid-3.1</ulink> and older are available: </para><itemizedlist><listitem override="none"><para><ulink url="https://wiki.squid-cache.org/Features/SmpScale/ConfigExamples/MultiCpuSystem#">ConfigExamples/MultiCpuSystem</ulink> </para></listitem><listitem override="none"><para><ulink url="https://wiki.squid-cache.org/Features/SmpScale/ConfigExamples/ExtremeCarpFrontend#">ConfigExamples/ExtremeCarpFrontend</ulink> </para></listitem></itemizedlist></section></section><section><title>SMP architecture layers</title><para>After a long discussion, project developers have agreed that different design choices are likely at different Squid architecture layers. This section attempts to document these SMP-relevant layers. However, it is not clear whether there is an agreement regarding many details beyond the basics of the top and bottom layers. </para><section><title>1. Top Layer: workers</title><para>Multiple Squid worker processes and/or threads. Each worker is responsible for a subset of transactions, with little interaction between workers except for caching. There is a master process or thread for worker coordination. </para></section><section><title>2. Mid Layer: threaded processes</title><para>The mid-layer of the final SMP architecture is to be a process handling a mixture of operations in multiple threads. The existing binary needs a lot of work done to identify what portions are suitable for becoming individual threads and cleanup of the code for that to be implemented. </para><para>Some components may be isolated to become individual application processes. </para><para>This portion of the development is known to be very large and is expected to occur gradually over many releases. Work on this began with early Squid-2 releases and is ongoing. </para><para>The interfaces for helper processes handling disk deletions, ICMP, authentication and URL re-writing are part of this layer. </para></section><section><title>3. Lowest Layer: multiple event and signal driven threads</title><para>At the lowest layer within each thread of operations the current design of non-blocking events is to be retained. This has proven to be of great efficiency in scaling already. </para><para>Work on this is joint with work on the mid-layer. Identifying groups of events which can be run as a thread with minimal interaction with other threads. The approach of having each thread pull data in and run many segments of events on it is preferable to starting and stopping threads frequently. </para><para>Currently existing pathways of processing need to be audited and some may need alterations to reduce resource interactions. </para></section></section><section><title>Progress and Dependencies</title><para>This constitutes how the Squid-3 maintainer sees the current work flowing towards SMP support. There are likely to be problems and unexpected things encountered at every turn, starting with the disagreements on this view itself. </para><itemizedlist><listitem override="none"><para><ulink url="http://www.squid-cache.org/Devel/papers/threading-notes.txt"/> while old still contains a good and valid analysis of the SMP problems inside Squid which must be hurdled. </para></listitem></itemizedlist><para>We have broken the SMP requirements of Squid into a series of smaller work units and are trying to get the following completed as spare time permits. </para><orderedlist numeration="arabic"><listitem><para>The modularization of Squid code into compact logical work units suitable for SMP consideration. Tracked as <ulink url="https://wiki.squid-cache.org/Features/SmpScale/Features/SourceLayout#">Features/SourceLayout</ulink> </para></listitem><listitem><para>Those resulting module libraries then need to be made fully Async <ulink url="https://wiki.squid-cache.org/Features/SmpScale/Features/NativeAsyncCalls#">Features/NativeAsyncCalls</ulink> jobs. </para></listitem><listitem><para>Finally those resulting jobs made into SMP threads that can utilize one of many CPUs.  Code checked for thread safety and efficient resource handling.  Recalling that a <emphasis>Job</emphasis> requires its <emphasis>Calls</emphasis> to happen in sequence. </para><itemizedlist><listitem><para>Probably wrong but it seems that <emphasis>AsyncCalls</emphasis> may float between CPU as long as they retain the sequential nature.  <emphasis>AsyncJobs</emphasis> may be run fully parallel interleaved, perhapse with some locking where one Job depends on another. </para></listitem></itemizedlist></listitem></orderedlist><para>Some other features are aimed at reducing the blocker problems for SMP. Not exactly forward steps along the SMP capability pathway, but required to make those steps possible. </para><itemizedlist><listitem><para><ulink url="https://wiki.squid-cache.org/Features/SmpScale/Features/NoCentralStoreIndex#">Features/NoCentralStoreIndex</ulink> </para></listitem><listitem><para><ulink url="https://wiki.squid-cache.org/Features/SmpScale/Features/CommCleanup#">Features/CommCleanup</ulink> </para></listitem><listitem><para><ulink url="https://wiki.squid-cache.org/Features/SmpScale/Features/ClientSideCleanup#">Features/ClientSideCleanup</ulink> </para></listitem><listitem><para>Forwarding API also needs work, but has no tracker feature yet. </para></listitem></itemizedlist></section><section><title>Sharing of Resources and Services</title><para>To safely share a resource or service among workers, that resource or service must be either rewritten in a sharing-safe way or all its uses must be globally locked. The former is often difficult, and the latter is often inefficient. Moreover, due to numerous inter-dependencies, it is often impossible to rewrite just one given resource/service algorithm (because all underlying code must be sharing-safe) or globally lock it (because that would lead to deadlocks). </para><para>Resources and services that are currently isolated but may benefit from sharing include: </para><itemizedlist><listitem><para>ipcache and fqdncache  (may benefit from merging so that only one DNS cache needs to be shared) </para></listitem><listitem><para>ufs-based caching storage (see above) </para></listitem><listitem><para>statistics (current cache manager implementation shares worker stats from the admin point of view) </para></listitem><listitem><para>memory manager </para></listitem><listitem><para>configuration objects </para></listitem></itemizedlist><para>Low-level components that may need to be rewritten in a sharing-safe way or replaced include: </para><itemizedlist><listitem><para>hash_link </para></listitem><listitem><para>dlink_list </para></listitem><listitem><para>FD / fde handling </para></listitem><listitem><para>memory buffers </para></listitem><listitem><para>String </para></listitem><listitem><para>any function, method, or class with static variables. </para></listitem></itemizedlist><para>One possibility often spoken of is to replace one or more of the low-level components with a public implementation having better thread-safe implementation (usually referring to the linked-list and hash algorithms).  Deep testing will be needed however to check for suitable speedy and efficient versions. </para></section><section><title>Troubleshooting</title><section><title>Ipc::Mem::Segment::create failed to shm_open(...): (13) Permission denied</title><para>On Linux the page pool should &quot;just work&quot;. However it is still dependent on the SHM device mapping being initialized. </para><para>Add the following line to your <emphasis role="strong">/etc/fstab file</emphasis>: </para><screen><![CDATA[shm        /dev/shm    tmpfs    nodev,nosuid,noexec    0    0]]></screen><para>After that use (as root): </para><screen><![CDATA[mount shm]]></screen></section><section><title>Ipc::Mem::Segment::create failed to shm_open(...): (2) No such file or directory</title><para>see above. </para></section><section><title>Cannot bind socket FD NN to [::]: (13) Permission denied</title><itemizedlist><listitem override="none"><para><inlinemediaobject><imageobject><imagedata depth="16" fileref="https://wiki.squid-cache.org/wiki/squidtheme/img/icon-info.png" width="16"/></imageobject><textobject><phrase>{i}</phrase></textobject></inlinemediaobject> Also may display as <emphasis role="strong">Cannot bind socket FD NN to 0.0.0.0: (13) Permission denied</emphasis> if you disabled IPv6. </para></listitem></itemizedlist><para>Squid registers its IPC channel sockets (Unix Domain Sockets or UDS) in the <emphasis role="strong">localstatedir</emphasis>.  For standard installs, this is usually <emphasis role="strong">/var/run/squid</emphasis>.  If you installed Squid to a different directory, see the --localstatedir ./configure option. </para><para>Check the permissions on <emphasis role="strong">/var/run/squid</emphasis> (or whatever your <emphasis>localstatedir</emphasis> is). The <emphasis role="strong">localstatedir</emphasis> directory requires ownership by the Squid user for SMP Squid to work. </para></section><section><title>write failure (40) Message too long</title><para>Squid workers exchange Unix Domain Sockets (UDS) messages (not to be confused with UDP messages or System V IPC messages). These messages should be smaller than 16KB in size, but even that creates problems in some environments because of very low default UDS message size and buffer size limits. </para><para>Usually, the limits can be adjusted using sysctl but exact control names are not documented and vary from one OS to another. Here is one known variant with recommended <emphasis>minimum</emphasis> settings (please add more if you know them!): </para><screen><![CDATA[net.local.dgram.recvspace: 262144
net.local.dgram.maxdgram: 16384]]></screen><!--rule (<hr>) is not applicable to DocBook--><para> <ulink url="https://wiki.squid-cache.org/Features/SmpScale/CategoryFeature#">CategoryFeature</ulink> <ulink url="https://wiki.squid-cache.org/Features/SmpScale/CategoryWish#">CategoryWish</ulink> </para></section></section></section></article>